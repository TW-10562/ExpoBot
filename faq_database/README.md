# FAQ Database Cache System

A high-performance FAQ caching system built with ChromaDB, sentence transformers, and FastAPI. This system provides intelligent question-answering capabilities through semantic search and cross-encoder reranking.

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Architecture](#architecture)
- [Installation](#installation)
- [Configuration](#configuration)
- [Usage](#usage)
- [API Endpoints](#api-endpoints)
- [Pipeline Details](#pipeline-details)
- [Project Structure](#project-structure)
- [Development](#development)
- [Troubleshooting](#troubleshooting)

## ğŸ¯ Overview

The FAQ Database Cache System is designed to provide fast, accurate responses to frequently asked questions by leveraging:

- **Semantic Search**: Uses Japanese sentence-BERT embeddings for understanding question intent
- **Vector Database**: ChromaDB for efficient similarity search
- **Reranking**: Cross-encoder model for precise relevance scoring
- **REST API**: FastAPI-based service for easy integration
- **Auto-Reconstruction**: Automatic database initialization from Excel files

## âœ¨ Features

- ğŸš€ **High Performance**: Sub-second query response times
- ğŸ¯ **Semantic Understanding**: Finds relevant FAQs even with different wording
- ğŸ”„ **Auto-Sync**: Automatically reconstructs database from Excel on startup
- ğŸ“Š **Dual Scoring**: Vector similarity + cross-encoder reranking
- ğŸ›¡ï¸ **Threshold Control**: Configurable confidence thresholds
- ğŸ” **Debug Mode**: Detailed logging for development
- ğŸ’¾ **Persistent Storage**: ChromaDB for data persistence
- ğŸŒ **REST API**: Easy integration with any application

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Query  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FAQ Cache API (FastAPI)    â”‚
â”‚  Port: 8001                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Query Pipeline              â”‚
â”‚  1. Embedding Generation     â”‚
â”‚  2. Vector Search (ANN)      â”‚
â”‚  3. Cross-Encoder Reranking  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ChromaDB                    â”‚
â”‚  - FAQ Questions (vectors)   â”‚
â”‚  - FAQ Answers (metadata)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

1. **Embedding Model**: `sonoisa/sentence-bert-base-ja-mean-tokens-v2`
   - Japanese-optimized sentence transformer
   - 768-dimensional embeddings

2. **Cross-Encoder**: `hotchpotch/japanese-reranker-cross-encoder-large-v1`
   - Precise relevance scoring
   - Final confidence validation

3. **Vector Database**: ChromaDB
   - Efficient similarity search
   - Persistent storage
   - Collection-based organization

## ğŸ“¦ Installation

### Prerequisites

- Python 3.8+
- pip or conda
- 4GB+ RAM (for models)
- 2GB+ disk space

### Setup

1. **Clone the repository** (if not already done):
```bash
cd /path/to/aviary-lite/faq_database
```

2. **Install dependencies**:
```bash
pip install -r requirements.txt
```

3. **Prepare FAQ data**:
   - Place your FAQ Excel file in `files/` directory
   - Default filename: `faq_10.xlsx`
   - Required columns: `question`, `answer`

4. **First-time setup**:
```bash
python main.py
```

This will:
- Download required models (~2GB)
- Create ChromaDB database
- Load FAQs from Excel
- Start the API server

## âš™ï¸ Configuration

### Environment Variables

```bash
# API Port (default: 8001)
export FAQ_CACHE_PORT=8001

# Excel File Path (default: files/faq_10.xlsx)
export FAQ_EXCEL_PATH=files/faq_10.xlsx

# Debug Mode (default: False)
export DEBUG=True
```

### Threshold Configuration

Thresholds can be configured per-request or globally:

```python
# In your API requests
{
  "query": "your question",
  "vector_similarity_threshold": 0.3,    # Default: 0.3 (lower = more lenient)
  "cross_encoder_threshold": 0.1         # Default: 0.1 (lower = more lenient)
}
```

**Threshold Guidelines**:
- **Vector Similarity**: 0.0-1.0 (cosine similarity)
  - 0.8+: Very high similarity (same question)
  - 0.5-0.8: Related questions
  - 0.3-0.5: Somewhat related
  - <0.3: Different topics

- **Cross-Encoder**: -1.0 to 1.0 (relevance score)
  - 0.5+: Highly relevant
  - 0.1-0.5: Moderately relevant
  - <0.1: Low relevance

## ğŸš€ Usage

### Starting the Server

```bash
python main.py
```

Output:
```
============================================================
FAQ Database Initialization
============================================================

ğŸ”„ Reconstructing FAQ database from Excel file...
ğŸ“‚ Excel file: files/faq_10.xlsx
âœ… Loaded 123 FAQ entries from Excel
ğŸ”„ Generating embeddings...
âœ… Database reconstructed successfully!
   - Collection: faq_collection
   - Total entries: 123

============================================================
Starting FAQ Cache API Server
============================================================
ğŸš€ Server running at: http://localhost:8001
ğŸ“š API Documentation: http://localhost:8001/docs
```

### Basic Query Example

**Python**:
```python
import requests

response = requests.post(
    "http://localhost:8001/query",
    json={
        "query": "æœ‰çµ¦ä¼‘æš‡ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
        "vector_similarity_threshold": 0.3,
        "cross_encoder_threshold": 0.1
    }
)

result = response.json()

if result["cache_hit"]:
    print(f"Question: {result['question']}")
    print(f"Answer: {result['answer']}")
    print(f"Confidence: {result['confidence']['cross_encoder_score']}")
else:
    print("No matching FAQ found")
```

**cURL**:
```bash
curl -X POST "http://localhost:8001/query" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "æœ‰çµ¦ä¼‘æš‡ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
    "vector_similarity_threshold": 0.3,
    "cross_encoder_threshold": 0.1
  }'
```

**Response**:
```json
{
  "cache_hit": true,
  "question": "æœ‰çµ¦ä¼‘æš‡ã®å–å¾—æ–¹æ³•ã¯ï¼Ÿ",
  "answer": "æœ‰çµ¦ä¼‘æš‡ã¯å‹¤æ€ ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ç”³è«‹ã§ãã¾ã™...",
  "confidence": {
    "vector_similarity": 0.85,
    "cross_encoder_score": 0.92
  },
  "query_time_ms": 145.2
}
```

## ğŸ“¡ API Endpoints

### Core Endpoints

#### `POST /query` - Query FAQ System
Query the FAQ database with semantic search.

**Request**:
```json
{
  "query": "string (required)",
  "vector_similarity_threshold": 0.3,
  "cross_encoder_threshold": 0.1
}
```

**Response**:
```json
{
  "cache_hit": true,
  "question": "string",
  "answer": "string",
  "confidence": {
    "vector_similarity": 0.85,
    "cross_encoder_score": 0.92
  },
  "query_time_ms": 145.2
}
```

#### `GET /health` - Health Check
Check if the service is running.

**Response**:
```json
{
  "status": "healthy",
  "service": "FAQ Cache API"
}
```

#### `GET /status` - Database Status
Get detailed information about the database.

**Response**:
```json
{
  "status": "ready",
  "database": {
    "path": "./database/chroma_db",
    "exists": true,
    "collections": ["faq_collection"],
    "total_entries": 123
  },
  "models": {
    "embedding_model": "sonoisa/sentence-bert-base-ja-mean-tokens-v2",
    "cross_encoder": "hotchpotch/japanese-reranker-cross-encoder-large-v1",
    "embedding_dimension": 768
  }
}
```

### Management Endpoints

#### `POST /reconstruct` - Rebuild Database
Reconstruct the database from Excel file.

**Request**:
```json
{
  "excel_path": "files/faq_10.xlsx",
  "collection_name": "faq_collection"
}
```

**Response**:
```json
{
  "status": "success",
  "message": "Database reconstructed successfully",
  "entries_processed": 123,
  "collection_name": "faq_collection"
}
```

#### `POST /save` - Add Single FAQ
Add or update a single FAQ entry.

**Request**:
```json
{
  "question": "æ–°ã—ã„è³ªå•",
  "answer": "æ–°ã—ã„å›ç­”",
  "collection_name": "faq_collection"
}
```

#### `POST /delete` - Delete FAQ Entry
Delete a FAQ entry by ID.

**Request**:
```json
{
  "entry_id": "unique_id_here",
  "collection_name": "faq_collection"
}
```

#### `POST /reset` - Reset Database
Delete all data and reset the database.

**Request**:
```json
{
  "collection_name": "faq_collection",
  "confirm": true
}
```

### Debug Endpoints

#### `GET /debug/collections` - List Collections
Get all collection names in the database.

#### `GET /debug/collection/{name}` - Collection Details
Get detailed information about a specific collection.

#### `POST /debug/search` - Raw Vector Search
Perform raw vector search without reranking (for debugging).

## ğŸ”¬ Pipeline Details

### Query Processing Flow

1. **Input Validation**
   - Validate query string
   - Check thresholds are in valid range

2. **Embedding Generation**
   ```python
   query_embedding = embedding_model.encode(user_query)
   # Output: 768-dimensional vector
   ```

3. **Vector Search (ANN)**
   ```python
   candidates = chromadb.query(
       query_embeddings=[query_embedding],
       n_results=top_k  # Default: 3
   )
   # Returns top-k most similar questions
   ```

4. **Cross-Encoder Reranking**
   ```python
   scores = cross_encoder.predict([
       (user_query, candidate_question)
       for candidate_question in candidates
   ])
   # Returns relevance scores for each candidate
   ```

5. **Threshold Filtering**
   ```python
   if vector_similarity >= threshold_1 and 
      cross_encoder_score >= threshold_2:
       return FAQ_HIT
   else:
       return CACHE_MISS
   ```

### Models Used

| Component | Model | Size | Language |
|-----------|-------|------|----------|
| Embedding | `sonoisa/sentence-bert-base-ja-mean-tokens-v2` | ~500MB | Japanese |
| Reranker | `hotchpotch/japanese-reranker-cross-encoder-large-v1` | ~1.2GB | Japanese |

### Performance Metrics

- **Query Latency**: 100-300ms (depending on hardware)
- **Throughput**: ~10-50 queries/second
- **Memory Usage**: ~3GB (with models loaded)
- **Disk Usage**: ~2GB (models) + database size

## ğŸ“ Project Structure

```
faq_database/
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ main.py                   # Application entry point
â”œâ”€â”€ pipeline.py              # Core query pipeline logic
â”œâ”€â”€ requirements.txt         # Python dependencies
â”‚
â”œâ”€â”€ api/                     # FastAPI application
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cache_api.py        # Main API application
â”‚   â””â”€â”€ schema.py           # Pydantic models
â”‚
â”œâ”€â”€ database/               # ChromaDB storage
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ chroma_repository.py  # Database interface
â”‚   â””â”€â”€ chroma_db/          # Persistent database (auto-created)
â”‚
â”œâ”€â”€ services/              # API route handlers
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ health.py         # Health check endpoint
â”‚   â”œâ”€â”€ query.py          # FAQ query endpoint
â”‚   â”œâ”€â”€ save.py           # Add FAQ endpoint
â”‚   â”œâ”€â”€ delete.py         # Delete FAQ endpoint
â”‚   â”œâ”€â”€ status.py         # Status endpoint
â”‚   â”œâ”€â”€ reset.py          # Reset database endpoint
â”‚   â”œâ”€â”€ reconstruct.py    # Rebuild database endpoint
â”‚   â””â”€â”€ debug.py          # Debug endpoints
â”‚
â”œâ”€â”€ src/                  # Core logic
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ann.py           # ANN search utilities
â”‚   â”œâ”€â”€ data_loader.py   # Excel data loading
â”‚   â”œâ”€â”€ embedding.py     # Embedding generation
â”‚   â”œâ”€â”€ hotchpotch.py    # Model configuration
â”‚   â””â”€â”€ db_reconstruction.py  # Database rebuild logic
â”‚
â””â”€â”€ files/               # Data files
    â””â”€â”€ faq_10.xlsx     # FAQ data (Excel format)
```

## ğŸ› ï¸ Development

### Running in Development Mode

```bash
# Enable debug logging
export DEBUG=True

# Run with auto-reload
uvicorn api.cache_api:app --reload --port 8001
```

### Running Tests

```bash
# Install test dependencies
pip install pytest pytest-asyncio httpx

# Run tests
pytest tests/
```

### Adding New FAQs

**Option 1: Via Excel File**
1. Edit `files/faq_10.xlsx`
2. Add rows with `question` and `answer` columns
3. Restart server or call `/reconstruct` endpoint

**Option 2: Via API**
```bash
curl -X POST "http://localhost:8001/save" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "æ–°ã—ã„è³ªå•",
    "answer": "æ–°ã—ã„å›ç­”"
  }'
```

### Excel File Format

The Excel file should have the following structure:

| question | answer |
|----------|--------|
| æœ‰çµ¦ä¼‘æš‡ã®å–å¾—æ–¹æ³•ã¯ï¼Ÿ | å‹¤æ€ ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ç”³è«‹ã§ãã¾ã™... |
| å¥åº·è¨ºæ–­ã¯ã„ã¤ã§ã™ã‹ï¼Ÿ | å¹´ã«1å›ã€4æœˆé ƒã«å®Ÿæ–½ã•ã‚Œã¾ã™... |
| ... | ... |

**Requirements**:
- Column names must be exactly `question` and `answer`
- First row must contain headers
- No empty rows between FAQs

## ğŸ› Troubleshooting

### Common Issues

#### 1. Models Not Downloading

**Problem**: Models fail to download from Hugging Face.

**Solution**:
```bash
# Set Hugging Face cache directory
export HF_HOME=/path/to/cache

# Or manually download models
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('sonoisa/sentence-bert-base-ja-mean-tokens-v2')
```

#### 2. Port Already in Use

**Problem**: `Address already in use` error.

**Solution**:
```bash
# Find and kill process using port 8001
lsof -ti:8001 | xargs kill -9

# Or use a different port
export FAQ_CACHE_PORT=8002
python main.py
```

#### 3. Out of Memory

**Problem**: System runs out of memory.

**Solution**:
- Reduce batch size in reconstruction
- Use smaller models
- Increase system swap space

#### 4. Excel File Not Found

**Problem**: `Excel file not found` on startup.

**Solution**:
```bash
# Check file path
ls -la files/faq_10.xlsx

# Or set custom path
export FAQ_EXCEL_PATH=/path/to/your/faq.xlsx
python main.py
```

#### 5. Low Accuracy Results

**Problem**: FAQ system returns irrelevant results.

**Solutions**:
- **Increase thresholds**: Set higher values for better precision
  ```json
  {
    "vector_similarity_threshold": 0.5,
    "cross_encoder_threshold": 0.3
  }
  ```
- **Add more FAQs**: Improve coverage with more examples
- **Refine questions**: Make FAQ questions more specific

### Debug Mode

Enable detailed logging:

```bash
export DEBUG=True
python main.py
```

This will show:
- Embedding generation details
- Vector search results
- Cross-encoder scores
- Timing information

### Checking Logs

```bash
# View API logs
tail -f logs/faq_cache.log

# Check ChromaDB logs
tail -f database/chroma_db/chroma.log
```

## ğŸ“Š Performance Optimization

### Tips for Better Performance

1. **Use SSD**: Store ChromaDB on SSD for faster access
2. **Increase RAM**: More RAM = faster model loading
3. **Batch Processing**: Process multiple queries in batch
4. **GPU Acceleration**: Use CUDA for faster embeddings
5. **Index Tuning**: Adjust ChromaDB index parameters

### GPU Support

To use GPU acceleration:

```bash
# Install PyTorch with CUDA
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Verify GPU is detected
python -c "import torch; print(torch.cuda.is_available())"
```

## ğŸ”’ Security Considerations

- **API Access**: Add authentication middleware if exposing publicly
- **Rate Limiting**: Implement rate limiting for production use
- **Input Validation**: All inputs are validated via Pydantic models
- **CORS**: Configure CORS settings based on your needs

## ğŸ“ License

This project is part of the Aviary-Lite system. Please refer to the main project license.

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“ Support

For issues and questions:
- Open an issue on GitHub
- Check existing issues for solutions
- Review the troubleshooting section

## ğŸ“ Credits

### Models
- **Sentence-BERT**: [sonoisa/sentence-bert-base-ja-mean-tokens-v2](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2)
- **Cross-Encoder**: [hotchpotch/japanese-reranker-cross-encoder-large-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-large-v1)

### Technologies
- **ChromaDB**: Vector database
- **FastAPI**: Web framework
- **Sentence Transformers**: Embedding models
- **Hugging Face**: Model hosting

---

**Version**: 1.0.0  
**Last Updated**: October 2025  
**Maintainer**: Aviary-AI Team
